apiVersion: "serving.kubeflow.org/v1beta1"
kind: "InferenceService"
metadata:
  name: torchserve-custom-gpu
spec:
  predictor:
    containers:
    - image: {username}/torchserve:latest-gpu
      name: torchserve-container
      ports:
        - containerPort: 8080
      resources:
        limits:
          nvidia.com/gpu: 1
   