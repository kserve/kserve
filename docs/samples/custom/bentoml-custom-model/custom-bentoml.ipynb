{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on a InferenceService using BentoML\n",
    "\n",
    "\n",
    "The notebook shows how to use BentoML to deploy InferenceService with a custom model.\n",
    "\n",
    "\n",
    "[BentoML](https://bentoml.org) is an open-source platform for high-performance ML model serving, which supports all major machine learning frameworks including Keras, Tensorflow, PyTorch, Fast.ai, XGBoost and etc.\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "* Your ~/.kube/config should point to a cluster with KFServing installed.\n",
    "* Your cluster's Istio Ingress gateway must be network accessible.\n",
    "* docker and docker hub must be properly configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bentoml\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define ML service with BentoML**\n",
    "\n",
    "These code defines a prediction service that requires a scikit-learn model, and asks BentoML to figure out the required PyPI pip packages automatically. It also defined an API, which is the entry point for accessing this prediction service. And the API is expecting a pandas.DataFrame object as its input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.handlers import DataframeHandler\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the trained model to local disk with BentoML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use bentoml CLI to test prediction with sample data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input '[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a custom InferenceService with BentoML using the command line\n",
    "\n",
    "\n",
    "This example includes additional files for KFServing V1 prediction protocol.\n",
    "\n",
    "*Better support for KFserving and its V2 prediction protocol is coming with BentoML.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {saved_path}/app.py\n",
    "\n",
    "import os\n",
    "from flask import Flask, request\n",
    "\n",
    "from bentoml import load\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "bento_service = load('.')\n",
    "api = bento_service.get_service_api('predict')\n",
    "\n",
    "\n",
    "@app.route('/v1/models/iris-classifier:predict')\n",
    "def predict():\n",
    "    return api.handle_request(request)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {saved_path}/Dockerfile-kfserving\n",
    "\n",
    "FROM continuumio/miniconda3:4.7.12\n",
    "\n",
    "ENTRYPOINT [ \"/bin/bash\", \"-c\" ]\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "RUN set -x \\\\\n",
    "     && apt-get update \\\\\n",
    "     && apt-get install --no-install-recommends --no-install-suggests -y libpq-dev build-essential \\\\\n",
    "     && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# pre-install BentoML base dependencies\n",
    "RUN conda install pip numpy scipy \\\\\n",
    "      && pip install gunicorn\n",
    "\n",
    "# copy over model files\n",
    "COPY . /bento\n",
    "WORKDIR /bento\n",
    "\n",
    "# run user defined setup script\n",
    "RUN if [ -f /bento/setup.sh ]; then /bin/bash -c /bento/setup.sh; fi\n",
    "\n",
    "# update conda base env\n",
    "RUN conda env update -n base -f /bento/environment.yml\n",
    "ARG PIP_TRUSTED_HOST\n",
    "ARG PIP_INDEX_URL\n",
    "RUN pip install -r /bento/requirements.txt\n",
    "\n",
    "# Install additional pip dependencies inside bundled_pip_dependencies dir\n",
    "RUN if [ -f /bento/bentoml_init.sh ]; then /bin/bash -c /bento/bentoml_init.sh; fi\n",
    "\n",
    "# Run Gunicorn server with path to module.\n",
    "CMD [\"exec gunicorn --bind :$PORT --workers 1 --threads 8 app:app\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure docker_username has correct value\n",
    "docker_username=DOCKER_USERNAME\n",
    "model_path=$(bentoml get IrisClassifier:latest -q | jq -r \".uri.uri\")\n",
    "\n",
    "docker build -t $docker_username/kfserving-iris-classifier $model_path/Dockerfile-kfserving\n",
    "\n",
    "docker push $docker_username/kfserving-iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Update the docker image tag inside InferenceServer yaml definition and apply to the cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Ensure docker_username has correct value\n",
    "docker_username=DOCKER_USERNAME \n",
    "sed 's/{docker_username}/'\"$docker_username\"'/g' custom.yaml\n",
    "kubectl apply -f custom.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prediction\n",
    "\n",
    "*Note: Use kfserving-ingressgateway as your INGRESS_GATEWAY if you are deploying KFServing as part of Kubeflow install, and not independently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=iris-classifier\n",
    "INGRESS_GATEWAY=istio-ingressgateway\n",
    "CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "\n",
    "curl -v -H \"Host: ${SERVICE_HOSTNAME}\" \\\n",
    "  --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "  http://$CLUSTER_IP/v1/models/${MODEL_NAME}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f custom.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
