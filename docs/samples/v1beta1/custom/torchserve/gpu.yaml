apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: torchserve-custom-gpu
spec:
  predictor:
    containers:
    - image: {username}/torchserve-gpu:latest
      name: torchserve-container
      resources:
        limits:
          nvidia.com/gpu: 1
