## KFServing Examples

### Deploy KFServing InferenceService with out of the box Predictor
[SKLearn Model](./docs/samples/sklearn)

[PyTorch Model](./docs/samples/pytorch)

[Tensorflow Model](./docs/samples/tensorflow)

[XGBoost Model](./docs/samples/xgboost)

[ONNX Model with ONNX Runtime](./docs/samples/onnx)

[TensorRT Model with NVIDIA's TensorRT Inference Server](./docs/samples/tensorrt)

### Deploy KFServing InferenceService with Transformer
[Image Transformer with PyTorch Predictor](./docs/samples/transformer/image_transformer)

### Deploy KFServing InferenceService with Explainer
[Alibi Image Explainer](./docs/samples/explanation/alibi/imagenet)

[Alibi Text Explainer](./docs/samples/explanation/alibi/moviesentiment)

[Alibi Tabular Explainer](./docs/samples/explanation/alibi/income)

### Deploy KFServing InferenceService with Cloud or PVC storage

[Models on S3](./docs/samples/s3)

[Models on PVC](./docs/samples/pvc)

[Models on Azure](./docs/samples/azure)

### Deploy KFServing InferenceService with Autoscaling, Canary Rollout and Other Integrations
[Autoscale inference workload on CPU/GPU](./docs/samples/autoscaling)

[InferenceService on GPU nodes](./docs/samples/accelerators)

[Canary Rollout](./docs/samples/rollouts)

[InferenceService with Kubeflow Pipeline](./docs/samples/pipelines)

[InferenceService with Request/Response Logger](./docs/samples/logger/basic)

[InferenceService with Kafka Event Source](./docs/samples/kafka)
