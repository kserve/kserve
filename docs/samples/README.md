## KFServing Examples

### Deploy KFServing InferenceService with out of the box Predictor
[SKLearn Model](./sklearn)

[PyTorch Model](./pytorch)

[Tensorflow Model](./tensorflow)

[XGBoost Model](./xgboost)

[ONNX Model with ONNX Runtime](./onnx)

[Simple String Model with NVIDIA Triton Inference Server](./triton/simple_string)

[Serve BERT Model with NVIDIA Triton Inference Server](./triton/bert)

### Deploy KFServing InferenceService with a custom Predictor

[Hello World Flask Server](./custom/hello-world)

[KFServing Custom Model](./custom/kfserving-custom-model)

[Prebuilt Image](./custom/prebuilt-image)

[BentoML](./bentoml)

### Deploy KFServing InferenceService with Transformer
[Image Transformer with PyTorch Predictor](./transformer/image_transformer)

### Deploy KFServing InferenceService with Explainer
[Alibi Image Explainer](./explanation/alibi/imagenet)

[Alibi Text Explainer](./explanation/alibi/moviesentiment)

[Alibi Tabular Explainer](./explanation/alibi/income)

### Deploy KFServing InferenceService with Cloud or PVC storage

[Models on S3](./s3)

[Models on PVC](./pvc)

[Models on Azure](./azure)

### Deploy KFServing InferenceService with Autoscaling, Canary Rollout and Other Integrations
[Autoscale inference workload on CPU/GPU](./autoscaling)

[InferenceService on GPU nodes](./accelerators)

[Canary Rollout](./rollouts)

[InferenceService with Kubeflow Pipeline](./pipelines)

[InferenceService with Request/Response Logger](./logger/basic)

[InferenceService with Kafka Event Source](./kafka)

[InferenceService on Kubeflow with Istio-Dex](./istio-dex)
