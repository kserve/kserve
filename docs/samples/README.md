## KFServing Examples

### Deploy KFServing InferenceService with out of the box Predictor
[SKLearn Model](./sklearn)

[PyTorch Model](./pytorch)

[Tensorflow Model](./tensorflow)

[XGBoost Model](./xgboost)

[ONNX Model with ONNX Runtime](./onnx)

[TensorRT Model with NVIDIA's TensorRT Inference Server](./tensorrt)

### Deploy KFServing InferenceService with a custom Predictor

[Hello World Flask Server](./custom/hello-world)

[KFServing Custom Model](./custom/kfserving-custom-model)

[Prebuilt Image](./custom/prebuilt-image)

### Deploy KFServing InferenceService with Transformer
[Image Transformer with PyTorch Predictor](./transformer/image_transformer)

### Deploy KFServing InferenceService with Explainer
[Alibi Image Explainer](./explanation/alibi/imagenet)

[Alibi Text Explainer](./explanation/alibi/moviesentiment)

[Alibi Tabular Explainer](./explanation/alibi/income)

### Deploy KFServing InferenceService with Cloud or PVC storage

[Models on S3](./s3)

[Models on PVC](./pvc)

[Models on Azure](./azure)

### Deploy KFServing InferenceService with Autoscaling, Canary Rollout and Other Integrations
[Autoscale inference workload on CPU/GPU](./autoscaling)

[InferenceService on GPU nodes](./accelerators)

[Canary Rollout](./rollouts)

[InferenceService with Kubeflow Pipeline](./pipelines)

[InferenceService with Request/Response Logger](./logger/basic)

[InferenceService with Kafka Event Source](./kafka)
