## KFServing Examples

### KFServing InferenceService with out of the box Predictor
[Deploy SKLearn Model with out-of-the-box InferenceService](./docs/samples/sklearn)

[Deploy PyTorch Model with out-of-the-box InferenceService](./docs/samples/pytorch)

[Deploy Tensorflow Model with out-of-the-box InferenceService](./docs/samples/tensorflow)

[Deploy XGBoost Model with out-of-the-box InferenceService](./docs/samples/xgboost)

[Deploy ONNX Model with ONNX Runtime InferenceService](./docs/samples/onnx)

[Deploy Deep Learning Models with NVIDIA's TensorRT InferenceService](./docs/samples/tensorrt)

### KFServing InferenceService with Transformer
[Deploy KFServing InferenceService with Transformer and Predictor](./docs/samples/transformer/image_transformer)

### KFServing InferenceService with Explainer
[Deploy KFServing InferenceService with Alibi Image Explainer](./docs/samples/explanation/alibi/imagenet)

[Deploy KFServing InferenceService with Alibi Text Explainer](./docs/samples/explanation/alibi/moviesentiment)

[Deploy KFServing InferenceService with Alibi Tabular Explainer](./docs/samples/explanation/alibi/income)

### KFServing InferenceService with Cloud or PVC storage

[Deploy KFServing InferenceService with Models on S3](./docs/samples/s3)

[Deploy KFServing InferenceService with Models on PVC](./docs/samples/pvc)

[Deploy KFServing InferenceService with Models on Azure](./docs/samples/azure)

### KFServing Autoscaling on CPU/GPU, Canary Rollout and other integrations 
[Autoscale KFServing InferenceService with your inference workload on CPU/GPU](./docs/samples/autoscaling)

[Deploy KFServing InferenceService on GPU nodes](./docs/samples/accelerators)

[Deploy KFServing InferenceService with Canary Rollout](./docs/samples/rollouts)

[Deploy KFServing InferenceService with Kubeflow Pipeline](./docs/samples/pipelines)

[Deploy KFServing InferenceService with Request/Response Logger](./docs/samples/logger/basic)

[Deploy KFServing InferenceService with Kafka Event Source](./docs/samples/kafka)
