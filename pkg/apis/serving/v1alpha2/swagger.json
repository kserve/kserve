{
  "swagger": "2.0",
  "info": {
    "description": "Python SDK for KFServing",
    "title": "KFServing",
    "version": "v0.1"
  },
  "paths": {},
  "definitions": {
    "knative.Condition": {
      "description": "Conditions defines a readiness condition for a Knative resource. See: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties",
      "type": "object",
      "required": [
        "type",
        "status"
      ],
      "properties": {
        "lastTransitionTime": {
          "description": "LastTransitionTime is the last time the condition transitioned from one status to another. We use VolatileTime in place of metav1.Time to exclude this from creating equality.Semantic differences (all other things held constant).",
          "$ref": "#/definitions/knative.VolatileTime"
        },
        "message": {
          "description": "A human readable message indicating details about the transition.",
          "type": "string"
        },
        "reason": {
          "description": "The reason for the condition's last transition.",
          "type": "string"
        },
        "severity": {
          "description": "Severity with which to treat failures of this type of condition. When this is not specified, it defaults to Error.",
          "type": "string"
        },
        "status": {
          "description": "Status of the condition, one of True, False, Unknown.",
          "type": "string"
        },
        "type": {
          "description": "Type of condition.",
          "type": "string"
        }
      }
    },
    "knative.VolatileTime": {
      "description": "VolatileTime wraps metav1.Time",
      "type": "object",
      "required": [
        "Time"
      ],
      "properties": {
        "Time": {
          "type": "string",
          "format": "date-time"
        }
      }
    },
    "v1alpha2.AlibiExplainerSpec": {
      "description": "AlibiExplainerSpec defines the arguments for configuring an Alibi Explanation Server",
      "type": "object",
      "required": [
        "type"
      ],
      "properties": {
        "config": {
          "description": "Inline custom parameter settings for explainer",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        },
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Defaults to latest Alibi Version",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of a trained explanation model",
          "type": "string"
        },
        "type": {
          "description": "The type of Alibi explainer",
          "type": "string"
        }
      }
    },
    "v1alpha2.CustomSpec": {
      "description": "CustomSpec provides a hook for arbitrary container configuration.",
      "type": "object",
      "required": [
        "container"
      ],
      "properties": {
        "container": {
          "$ref": "#/definitions/v1.Container"
        }
      }
    },
    "v1alpha2.DeploymentSpec": {
      "description": "DeploymentSpec defines the configuration for a given InferenceService service component",
      "type": "object",
      "properties": {
        "logger": {
          "description": "Activate request/response logging",
          "$ref": "#/definitions/v1alpha2.Logger"
        },
        "maxReplicas": {
          "description": "This is the up bound for autoscaler to scale to",
          "type": "integer",
          "format": "int32"
        },
        "minReplicas": {
          "description": "Minimum number of replicas, pods won't scale down to 0 in case of no traffic",
          "type": "integer",
          "format": "int32"
        },
        "parallelism": {
          "description": "Parallelism specifies how many requests can be processed concurrently, this sets the target concurrency for Autoscaling(KPA). For model servers that support tuning parallelism will use this value, by default the parallelism is the number of the CPU cores for most of the model servers.",
          "type": "integer",
          "format": "int32"
        },
        "serviceAccountName": {
          "description": "ServiceAccountName is the name of the ServiceAccount to use to run the service",
          "type": "string"
        }
      }
    },
    "v1alpha2.EndpointSpec": {
      "type": "object",
      "required": [
        "predictor"
      ],
      "properties": {
        "explainer": {
          "description": "Explainer defines the model explanation service spec, explainer service calls to predictor or transformer if it is specified.",
          "$ref": "#/definitions/v1alpha2.ExplainerSpec"
        },
        "predictor": {
          "description": "Predictor defines the model serving spec",
          "$ref": "#/definitions/v1alpha2.PredictorSpec"
        },
        "transformer": {
          "description": "Transformer defines the pre/post processing before and after the predictor call, transformer service calls to predictor service.",
          "$ref": "#/definitions/v1alpha2.TransformerSpec"
        }
      }
    },
    "v1alpha2.ExplainerSpec": {
      "description": "ExplainerSpec defines the arguments for a model explanation server, The following fields follow a \"1-of\" semantic. Users must specify exactly one spec.",
      "type": "object",
      "properties": {
        "alibi": {
          "description": "Spec for alibi explainer",
          "$ref": "#/definitions/v1alpha2.AlibiExplainerSpec"
        },
        "custom": {
          "description": "Spec for a custom explainer",
          "$ref": "#/definitions/v1alpha2.CustomSpec"
        },
        "logger": {
          "description": "Activate request/response logging",
          "$ref": "#/definitions/v1alpha2.Logger"
        },
        "maxReplicas": {
          "description": "This is the up bound for autoscaler to scale to",
          "type": "integer",
          "format": "int32"
        },
        "minReplicas": {
          "description": "Minimum number of replicas, pods won't scale down to 0 in case of no traffic",
          "type": "integer",
          "format": "int32"
        },
        "parallelism": {
          "description": "Parallelism specifies how many requests can be processed concurrently, this sets the target concurrency for Autoscaling(KPA). For model servers that support tuning parallelism will use this value, by default the parallelism is the number of the CPU cores for most of the model servers.",
          "type": "integer",
          "format": "int32"
        },
        "serviceAccountName": {
          "description": "ServiceAccountName is the name of the ServiceAccount to use to run the service",
          "type": "string"
        }
      }
    },
    "v1alpha2.InferenceService": {
      "description": "InferenceService is the Schema for the services API",
      "type": "object",
      "properties": {
        "apiVersion": {
          "description": "APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources",
          "type": "string"
        },
        "kind": {
          "description": "Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds",
          "type": "string"
        },
        "metadata": {
          "$ref": "#/definitions/v1.ObjectMeta"
        },
        "spec": {
          "$ref": "#/definitions/v1alpha2.InferenceServiceSpec"
        },
        "status": {
          "$ref": "#/definitions/v1alpha2.InferenceServiceStatus"
        }
      }
    },
    "v1alpha2.InferenceServiceList": {
      "description": "InferenceServiceList contains a list of Service",
      "type": "object",
      "required": [
        "items"
      ],
      "properties": {
        "apiVersion": {
          "description": "APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources",
          "type": "string"
        },
        "items": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/v1alpha2.InferenceService"
          },
          "x-kubernetes-list-type": "set"
        },
        "kind": {
          "description": "Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds",
          "type": "string"
        },
        "metadata": {
          "$ref": "#/definitions/v1.ListMeta"
        }
      }
    },
    "v1alpha2.InferenceServiceSpec": {
      "description": "InferenceServiceSpec defines the desired state of InferenceService",
      "type": "object",
      "required": [
        "default"
      ],
      "properties": {
        "canary": {
          "description": "Canary defines an alternate endpoints to route a percentage of traffic.",
          "$ref": "#/definitions/v1alpha2.EndpointSpec"
        },
        "canaryTrafficPercent": {
          "description": "CanaryTrafficPercent defines the percentage of traffic going to canary InferenceService endpoints",
          "type": "integer",
          "format": "int32"
        },
        "default": {
          "description": "Default defines default InferenceService endpoints",
          "$ref": "#/definitions/v1alpha2.EndpointSpec"
        }
      }
    },
    "v1alpha2.InferenceServiceStatus": {
      "description": "InferenceServiceStatus defines the observed state of InferenceService",
      "type": "object",
      "properties": {
        "address": {
          "description": "Ducktype for addressable",
          "$ref": "#/definitions/knative.Addressable"
        },
        "canary": {
          "description": "Statuses for the canary endpoints of the InferenceService",
          "type": "object",
          "additionalProperties": {
            "$ref": "#/definitions/v1alpha2.StatusConfigurationSpec"
          }
        },
        "canaryTraffic": {
          "description": "Traffic percentage that goes to canary services",
          "type": "integer",
          "format": "int32"
        },
        "conditions": {
          "description": "Conditions the latest available observations of a resource's current state.",
          "type": "array",
          "items": {
            "$ref": "#/definitions/knative.Condition"
          },
          "x-kubernetes-patch-merge-key": "type",
          "x-kubernetes-patch-strategy": "merge"
        },
        "default": {
          "description": "Statuses for the default endpoints of the InferenceService",
          "type": "object",
          "additionalProperties": {
            "$ref": "#/definitions/v1alpha2.StatusConfigurationSpec"
          }
        },
        "observedGeneration": {
          "description": "ObservedGeneration is the 'Generation' of the Service that was last processed by the controller.",
          "type": "integer",
          "format": "int64"
        },
        "traffic": {
          "description": "Traffic percentage that goes to default services",
          "type": "integer",
          "format": "int32"
        },
        "url": {
          "description": "URL of the InferenceService",
          "type": "string"
        }
      }
    },
    "v1alpha2.Logger": {
      "description": "Logger provides optional payload logging for all endpoints",
      "type": "object",
      "properties": {
        "mode": {
          "description": "What payloads to log",
          "type": "string"
        },
        "url": {
          "description": "URL to send request logging CloudEvents",
          "type": "string"
        }
      }
    },
    "v1alpha2.ONNXSpec": {
      "description": "ONNXSpec defines arguments for configuring ONNX model serving.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    },
    "v1alpha2.PredictorSpec": {
      "description": "PredictorSpec defines the configuration for a predictor, The following fields follow a \"1-of\" semantic. Users must specify exactly one spec.",
      "type": "object",
      "properties": {
        "custom": {
          "description": "Spec for a custom predictor",
          "$ref": "#/definitions/v1alpha2.CustomSpec"
        },
        "logger": {
          "description": "Activate request/response logging",
          "$ref": "#/definitions/v1alpha2.Logger"
        },
        "maxReplicas": {
          "description": "This is the up bound for autoscaler to scale to",
          "type": "integer",
          "format": "int32"
        },
        "minReplicas": {
          "description": "Minimum number of replicas, pods won't scale down to 0 in case of no traffic",
          "type": "integer",
          "format": "int32"
        },
        "onnx": {
          "description": "Spec for ONNX runtime (https://github.com/microsoft/onnxruntime)",
          "$ref": "#/definitions/v1alpha2.ONNXSpec"
        },
        "parallelism": {
          "description": "Parallelism specifies how many requests can be processed concurrently, this sets the target concurrency for Autoscaling(KPA). For model servers that support tuning parallelism will use this value, by default the parallelism is the number of the CPU cores for most of the model servers.",
          "type": "integer",
          "format": "int32"
        },
        "pytorch": {
          "description": "Spec for PyTorch predictor",
          "$ref": "#/definitions/v1alpha2.PyTorchSpec"
        },
        "serviceAccountName": {
          "description": "ServiceAccountName is the name of the ServiceAccount to use to run the service",
          "type": "string"
        },
        "sklearn": {
          "description": "Spec for SKLearn predictor",
          "$ref": "#/definitions/v1alpha2.SKLearnSpec"
        },
        "tensorflow": {
          "description": "Spec for Tensorflow Serving (https://github.com/tensorflow/serving)",
          "$ref": "#/definitions/v1alpha2.TensorflowSpec"
        },
        "triton": {
          "description": "Spec for Triton Inference Server (https://github.com/NVIDIA/triton-inference-server)",
          "$ref": "#/definitions/v1alpha2.TritonSpec"
        },
        "xgboost": {
          "description": "Spec for XGBoost predictor",
          "$ref": "#/definitions/v1alpha2.XGBoostSpec"
        }
      }
    },
    "v1alpha2.PyTorchSpec": {
      "description": "PyTorchSpec defines arguments for configuring PyTorch model serving.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "modelClassName": {
          "description": "Defaults PyTorch model class name to 'PyTorchModel'",
          "type": "string"
        },
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    },
    "v1alpha2.SKLearnSpec": {
      "description": "SKLearnSpec defines arguments for configuring SKLearn model serving.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    },
    "v1alpha2.StatusConfigurationSpec": {
      "description": "StatusConfigurationSpec describes the state of the configuration receiving traffic.",
      "type": "object",
      "properties": {
        "host": {
          "description": "Host name of the service",
          "type": "string"
        },
        "name": {
          "description": "Latest revision name that is in ready state",
          "type": "string"
        },
        "replicas": {
          "type": "integer",
          "format": "int32"
        }
      }
    },
    "v1alpha2.TensorflowSpec": {
      "description": "TensorflowSpec defines arguments for configuring Tensorflow model serving.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map.",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    },
    "v1alpha2.TransformerSpec": {
      "description": "TransformerSpec defines transformer service for pre/post processing",
      "type": "object",
      "properties": {
        "custom": {
          "description": "Spec for a custom transformer",
          "$ref": "#/definitions/v1alpha2.CustomSpec"
        },
        "logger": {
          "description": "Activate request/response logging",
          "$ref": "#/definitions/v1alpha2.Logger"
        },
        "maxReplicas": {
          "description": "This is the up bound for autoscaler to scale to",
          "type": "integer",
          "format": "int32"
        },
        "minReplicas": {
          "description": "Minimum number of replicas, pods won't scale down to 0 in case of no traffic",
          "type": "integer",
          "format": "int32"
        },
        "parallelism": {
          "description": "Parallelism specifies how many requests can be processed concurrently, this sets the target concurrency for Autoscaling(KPA). For model servers that support tuning parallelism will use this value, by default the parallelism is the number of the CPU cores for most of the model servers.",
          "type": "integer",
          "format": "int32"
        },
        "serviceAccountName": {
          "description": "ServiceAccountName is the name of the ServiceAccount to use to run the service",
          "type": "string"
        }
      }
    },
    "v1alpha2.TritonSpec": {
      "description": "TritonSpec defines arguments for configuring Triton Inference Server.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    },
    "v1alpha2.XGBoostSpec": {
      "description": "XGBoostSpec defines arguments for configuring XGBoost model serving.",
      "type": "object",
      "required": [
        "storageUri"
      ],
      "properties": {
        "nthread": {
          "description": "Number of thread to be used by XGBoost",
          "type": "integer",
          "format": "int32"
        },
        "resources": {
          "description": "Defaults to requests and limits of 1CPU, 2Gb MEM.",
          "$ref": "#/definitions/v1.ResourceRequirements"
        },
        "runtimeVersion": {
          "description": "Allowed runtime versions are specified in the inferenceservice config map",
          "type": "string"
        },
        "storageUri": {
          "description": "The location of the trained model",
          "type": "string"
        }
      }
    }
  }
}
