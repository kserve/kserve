# -- Common labels to add to all resources
commonLabels: {}

# -- Common annotations to add to all resources
commonAnnotations: {}

kserve:
  # -- Version of KServe LLM ISVC components
  version: &defaultVersion v0.15.2

  llmisvc:
    controller:
      # -- KServe LLM ISVC controller manager container image
      image: kserve/llmisvc-controller

      # -- KServe LLM ISVC controller container image tag
      tag: *defaultVersion

      # -- Specifies when to pull controller image from registry
      imagePullPolicy: "IfNotPresent"

      # -- Reference to one or more secrets to be used when pulling images
      # For more information, see [Pull an Image from a Private Registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/)
      #
      # For example:
      #  imagePullSecrets:
      #    - name: "image-pull-secret"
      imagePullSecrets: []

      # -- Optional additional labels to add to the controller deployment
      labels: {}

      # -- Optional additional labels to add to the controller Pods
      podLabels: {}

      # -- Optional additional annotations to add to the controller deployment
      annotations: {}

      # -- Optional additional annotations to add to the controller Pods
      podAnnotations: {}

      # -- Optional additional annotations to add to the controller service
      serviceAnnotations: {}

      # -- Pod Security Context
      # For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault

      # -- Container Security Context to be set on the controller component container
      # For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
      containerSecurityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault

      # -- Metrics bind address
      metricsBindAddress: "127.0.0.1"

      # -- Metrics bind port
      metricsBindPort: "8443"

      # -- The nodeSelector on Pods tells Kubernetes to schedule Pods on the nodes with matching labels
      # For more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
      #
      # For example:
      #   nodeSelector:
      #     kubernetes.io/arch: amd64
      nodeSelector: {}

      # -- A list of Kubernetes Tolerations, if required
      # For more information, see [Toleration v1 core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#toleration-v1-core)
      #
      # For example:
      #   tolerations:
      #   - key: foo.bar.com/role
      #     operator: Equal
      #     value: master
      #     effect: NoSchedule
      tolerations: []

      # -- A list of Kubernetes TopologySpreadConstraints, if required
      # For more information, see [Topology spread constraint v1 core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#topologyspreadconstraint-v1-core)
      #
      # For example:
      #   topologySpreadConstraints:
      #   - maxSkew: 2
      #     topologyKey: topology.kubernetes.io/zone
      #     whenUnsatisfiable: ScheduleAnyway
      #     labelSelector:
      #       matchLabels:
      #         app.kubernetes.io/instance: llmisvc-controller-manager
      #         app.kubernetes.io/component: controller
      topologySpreadConstraints: []

      # -- A Kubernetes Affinity, if required
      # For more information, see [Affinity v1 core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#affinity-v1-core)
      #
      # For example:
      #   affinity:
      #     nodeAffinity:
      #      requiredDuringSchedulingIgnoredDuringExecution:
      #        nodeSelectorTerms:
      #        - matchExpressions:
      #          - key: foo.bar.com/role
      #            operator: In
      #            values:
      #            - master
      affinity: {}

      # -- Resources to provide to the llmisvc controller pod
      #
      # For example:
      #  resources:
      #    limits:
      #      cpu: 100m
      #      memory: 300Mi
      #    requests:
      #      cpu: 100m
      #      memory: 300Mi
      #
      # For more information, see [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 100m
          memory: 300Mi

      # -- Number of replicas for the controller deployment
      replicas: 1

      # -- Deployment strategy
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0

      # -- Liveness probe configuration
      livenessProbe:
        enabled: true
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 8081

      # -- Readiness probe configuration
      readinessProbe:
        enabled: true
        initialDelaySeconds: 30
        periodSeconds: 5
        timeoutSeconds: 5
        failureThreshold: 5
        httpGet:
          path: /readyz
          port: 8081

      # -- Service configuration
      service:
        # -- Service type
        type: ClusterIP
        # -- Service port for metrics
        port: 8443
        # -- Service target port
        targetPort: metrics

      # -- Service account configuration
      serviceAccount:
        # -- Name of the service account to use
        # If not set, a name is generated using the deployment name
        name: ""

      # -- Environment variables to be set on the controller container
      env: []

      # -- Additional volumes to be mounted
      extraVolumes: []

      # -- Additional volume mounts
      extraVolumeMounts: []

      # -- Additional command line arguments
      extraArgs: []

      # -- Termination grace period in seconds
      terminationGracePeriodSeconds: 10

    # -- LLM Inference Service Configuration templates
    configs:
      # -- Common model configuration used by all templates
      model:
        # URI of the model - can be templated for runtime resolution
        uri: "{{`{{ .Spec.Model.URI }}`}}"
        # Name of the model - can be templated for runtime resolution
        name: "{{`{{ .Spec.Model.Name }}`}}"

      # -- Main VLLM template configuration
      template:
        # -- Whether to create the LLM template config
        enabled: true

        # -- Container image for VLLM
        image:
          repository: ghcr.io/llm-d/llm-d
          tag: "0.0.8"
          pullPolicy: IfNotPresent

        # -- Environment variables for VLLM container
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models

        # -- Container ports
        ports:
          - containerPort: 8000
            protocol: TCP

        # -- VLLM command arguments
        args:
          - --served-model-name
          - "{{ .Spec.Model.Name }}"
          - --port
          - "8000"
          - --disable-log-requests

        # -- Security context for VLLM container
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - MKNOD

        # -- Liveness probe configuration
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3

        # -- Readiness probe configuration
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 60

        # -- Volume mounts for VLLM container
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache

        # -- Pod volumes
        volumes:
          - emptyDir: {}
            name: home
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - emptyDir: {}
            name: model-cache

        # -- Termination grace period
        terminationGracePeriodSeconds: 30

      # -- Inference scheduler configuration
      scheduler:
        # -- Whether to create the scheduler config
        enabled: true

        # -- Container image for inference scheduler
        image:
          repository: ghcr.io/llm-d/llm-d-inference-scheduler
          tag: "0.0.4"
          pullPolicy: IfNotPresent

        # -- Container ports
        ports:
          - containerPort: 9002
            name: grpc
            protocol: TCP
          - containerPort: 9003
            name: grpc-health
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP

        # -- Scheduler command arguments
        args:
          - --poolName
          - "{{ ChildName .ObjectMeta.Name `-inference-pool` }}"
          - --poolNamespace
          - "{{ .ObjectMeta.Namespace }}"
          - --zap-encoder
          - json
          - --grpcPort
          - "9002"
          - --grpcHealthPort
          - "9003"

        # -- Environment variables for scheduler
        env:
          - name: ENABLE_LOAD_AWARE_SCORER
            value: "true"
          - name: ENABLE_PREFIX_AWARE_SCORER
            value: "true"
          - name: ENABLE_SESSION_AWARE_SCORER
            value: "true"
          - name: LOAD_AWARE_SCORER_WEIGHT
            value: "1"
          - name: PD_ENABLED
            value: "true"
          - name: PD_PROMPT_LEN_THRESHOLD
            value: "100"
          - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
            value: "true"
          - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
            value: "true"
          - name: PREFILL_ENABLE_SESSION_AWARE_SCORER
            value: "true"
          - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
            value: "1"
          - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
            value: "1"
          - name: PREFILL_SESSION_AWARE_SCORER_WEIGHT
            value: "1"
          - name: PREFIX_AWARE_SCORER_WEIGHT
            value: "2"
          - name: SESSION_AWARE_SCORER_WEIGHT
            value: "1"

        # -- Resources for scheduler container
        resources:
          requests:
            cpu: 256m
            memory: 500Mi

        # -- Security context for scheduler container
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault

        # -- Liveness probe configuration
        livenessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1

        # -- Readiness probe configuration
        readinessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: envoy.service.ext_proc.v3.ExternalProcessor
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1

        # -- Pool configuration
        pool:
          targetPortNumber: 8000
          extensionRef:
            name: "{{ ChildName .ObjectMeta.Name `-epp-service` }}"

        # -- Termination grace period
        terminationGracePeriodSeconds: 30

      # -- Decode template configuration
      decodeTemplate:
        # -- Whether to create the decode template config
        enabled: true

        # -- Container image for VLLM decode
        image:
          repository: ghcr.io/llm-d/llm-d
          tag: "0.0.8"
          pullPolicy: IfNotPresent

        # -- Environment variables for VLLM decode container
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models

        # -- Container ports for decode
        ports:
          - containerPort: 8001
            protocol: TCP

        # -- VLLM decode command arguments
        args:
          - --served-model-name
          - "{{ .Spec.Model.Name }}"
          - --port
          - "8001"
          - --disable-log-requests

        # -- VLLM decode command
        command:
          - vllm
          - serve
          - "{{ .Spec.Model.Name }}"

        # -- Security context for VLLM decode container
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - MKNOD

        # -- Liveness probe configuration for decode
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3

        # -- Readiness probe configuration for decode
        readinessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 60

        # -- Init container for routing sidecar
        initContainer:
          # -- Routing sidecar container image
          image:
            repository: ghcr.io/llm-d/llm-d-routing-sidecar
            tag: "0.0.6"
            pullPolicy: IfNotPresent

          # -- Routing sidecar ports
          ports:
            - containerPort: 8000
              protocol: TCP

          # -- Routing sidecar arguments
          args:
            - "--port=8000"
            - "--vllm-port=8001"

        # -- Volume mounts for VLLM decode container
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache

        # -- Pod volumes for decode
        volumes:
          - emptyDir: {}
            name: home
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - emptyDir: {}
            name: model-cache

        # -- Termination grace period for decode
        terminationGracePeriodSeconds: 30

      # -- Prefill template configuration
      prefillTemplate:
        # -- Whether to create the prefill template config
        enabled: true

        # -- Container image for VLLM prefill
        image:
          repository: ghcr.io/llm-d/llm-d
          tag: "0.0.8"
          pullPolicy: IfNotPresent

        # -- Environment variables for VLLM prefill container
        env:
          - name: HOME
            value: /home
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /models

        # -- Container ports for prefill
        ports:
          - containerPort: 8000
            protocol: TCP

        # -- VLLM prefill command arguments
        args:
          - --served-model-name
          - "{{ .Spec.Model.Name }}"
          - --port
          - "8000"
          - --disable-log-requests

        # -- VLLM prefill command
        command:
          - vllm
          - serve
          - "{{ .Spec.Model.Name }}"

        # -- Security context for VLLM prefill container
        securityContext:
          allowPrivilegeEscalation: false

        # -- Liveness probe configuration for prefill
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3

        # -- Readiness probe configuration for prefill
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 60

        # -- Volume mounts for VLLM prefill container
        volumeMounts:
          - mountPath: /home
            name: home
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /models
            name: model-cache

        # -- Pod volumes for prefill
        volumes:
          - emptyDir: {}
            name: home
          - emptyDir:
              medium: Memory
              sizeLimit: 1Gi
            name: dshm
          - emptyDir: {}
            name: model-cache

        # -- Termination grace period for prefill
        terminationGracePeriodSeconds: 30

      # -- Router route configuration
      routerRoute:
        # -- Whether to create the router route config
        enabled: true

        # -- HTTP route configuration
        http:
          # -- Parent gateway configuration
          parentRefs:
            - group: gateway.networking.k8s.io
              kind: Gateway
              name: "{{ .GlobalConfig.IngressGatewayName }}"
              namespace: kserve

          # -- Backend references configuration
          backendRefs:
            - group: inference.networking.x-k8s.io
              kind: InferencePool
              port: 8000
              weight: 1
              name: "{{ ChildName .ObjectMeta.Name `-inference-pool` }}"

          # -- Route matching configuration
          matches:
            - path:
                type: PathPrefix
                value: /

          # -- Timeout configuration
          timeouts:
            backendRequest: 0s
            request: 0s

      # -- Worker data parallel configurations
      workers:
        # -- Whether to create worker configs
        enabled: true

        # -- Decode worker data parallel config
        decode:
          enabled: true

          # -- Container image for VLLM decode worker
          image:
            repository: ghcr.io/llm-d/llm-d
            tag: "0.0.8"
            pullPolicy: IfNotPresent

          # -- Init container for routing sidecar
          initContainer:
            # -- Routing sidecar container image
            image:
              repository: ghcr.io/llm-d/llm-d-routing-sidecar
              tag: "0.0.6"
              pullPolicy: IfNotPresent

            # -- Routing sidecar ports
            ports:
              - containerPort: 8000
                protocol: TCP

            # -- Routing sidecar arguments
            args:
              - "--port=8000"
              - "--vllm-port=8001"

          # -- Environment variables for decode worker
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models

          # -- Container ports for decode worker
          ports:
            - containerPort: 8001
              protocol: TCP

          # -- Security context for decode worker
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - "IPC_LOCK"
                - "SYS_RAWIO"

          # -- Liveness probe configuration for decode worker
          livenessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 3

          # -- Readiness probe configuration for decode worker
          readinessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 60

          # -- Volume mounts for decode worker
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache

          # -- Pod volumes for decode worker
          volumes:
            - emptyDir: {}
              name: home
            - emptyDir:
                medium: Memory
                sizeLimit: 1Gi
              name: dshm
            - emptyDir: {}
              name: model-cache

          # -- Termination grace period for decode worker
          terminationGracePeriodSeconds: 30

        # -- Prefill worker data parallel config
        prefill:
          enabled: true

          # -- Container image for VLLM prefill worker
          image:
            repository: ghcr.io/llm-d/llm-d
            tag: "0.0.8"
            pullPolicy: IfNotPresent

          # -- Environment variables for prefill worker (copied from decode for consistency)
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models

          # -- Container ports for prefill worker
          ports:
            - containerPort: 8000
              protocol: TCP

          # -- Security context for prefill worker
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - "IPC_LOCK"
                - "SYS_RAWIO"

          # -- Liveness probe configuration for prefill worker
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 3

          # -- Readiness probe configuration for prefill worker
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 60

          # -- Volume mounts for VLLM prefill container
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache

          # -- Pod volumes for prefill
          volumes:
            - emptyDir: {}
              name: home
            - emptyDir:
                medium: Memory
                sizeLimit: 1Gi
              name: dshm
            - emptyDir: {}
              name: model-cache

          # -- Termination grace period for prefill worker
          terminationGracePeriodSeconds: 30

        # -- Generic worker data parallel config
        worker:
          enabled: true

          # -- Container image for VLLM generic worker
          image:
            repository: ghcr.io/llm-d/llm-d
            tag: "0.0.8"
            pullPolicy: IfNotPresent

          # -- Environment variables for generic worker
          env:
            - name: HOME
              value: /home
            - name: VLLM_LOGGING_LEVEL
              value: INFO
            - name: HF_HUB_CACHE
              value: /models

          # -- Container ports for generic worker
          ports:
            - containerPort: 8000
              protocol: TCP

          # -- Security context for generic worker
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - "IPC_LOCK"
                - "SYS_RAWIO"

          # -- Liveness probe configuration for generic worker
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 3

          # -- Readiness probe configuration for generic worker
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 60

          # -- Volume mounts for generic worker
          volumeMounts:
            - mountPath: /home
              name: home
            - mountPath: /dev/shm
              name: dshm
            - mountPath: /models
              name: model-cache

          # -- Pod volumes for generic worker
          volumes:
            - emptyDir: {}
              name: home
            - emptyDir:
                medium: Memory
                sizeLimit: 1Gi
              name: dshm
            - emptyDir: {}
              name: model-cache

          # -- Termination grace period for generic worker
          terminationGracePeriodSeconds: 30
