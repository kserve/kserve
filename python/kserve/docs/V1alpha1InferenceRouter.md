# V1alpha1InferenceRouter

InferenceRouter defines the router for each InferenceGraph node with one or multiple steps  ```yaml kind: InferenceGraph metadata:   name: canary-route spec:   nodes:     root:       routerType: Splitter       routes:       - service: mymodel1         weight: 20       - service: mymodel2         weight: 80 ```  ```yaml kind: InferenceGraph metadata:   name: abtest spec:   nodes:     mymodel:       routerType: Switch       routes:       - service: mymodel1         condition: \"{ .input.userId == 1 }\"       - service: mymodel2         condition: \"{ .input.userId == 2 }\" ```  Scoring a case using a model ensemble consists of scoring it using each model separately, then combining the results into a single scoring result using one of the pre-defined combination methods.  Tree Ensemble constitutes a case where simple algorithms for combining results of either classification or regression trees are well known. Multiple classification trees, for example, are commonly combined using a \"majority-vote\" method. Multiple regression trees are often combined using various averaging techniques. e.g tagging models with segment identifiers and weights to be used for their combination in these ways. ```yaml kind: InferenceGraph metadata:   name: ensemble spec:   nodes:     root:       routerType: Sequence       routes:       - service: feast       - nodeName: ensembleModel         data: $response     ensembleModel:       routerType: Ensemble       routes:       - service: sklearn-model       - service: xgboost-model ```  Scoring a case using a sequence, or chain of models allows the output of one model to be passed in as input to the subsequent models. ```yaml kind: InferenceGraph metadata:   name: model-chainer spec:   nodes:     root:       routerType: Sequence       routes:       - service: mymodel-s1       - service: mymodel-s2         data: $response       - service: mymodel-s3         data: $response ```  In the flow described below, the pre_processing node base64 encodes the image and passes it to two model nodes in the flow. The encoded data is available to both these nodes for classification. The second node i.e. dog-breed-classification takes the original input from the pre_processing node along-with the response from the cat-dog-classification node to do further classification of the dog breed if required. ```yaml kind: InferenceGraph metadata:   name: dog-breed-classification spec:   nodes:     root:       routerType: Sequence       routes:       - service: cat-dog-classifier       - nodeName: breed-classifier         data: $request     breed-classifier:       routerType: Switch       routes:       - service: dog-breed-classifier         condition: { .predictions.class == \"dog\" }       - service: cat-breed-classifier         condition: { .predictions.class == \"cat\" } ```
## Properties
Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**router_type** | **str** | RouterType  - &#x60;Sequence:&#x60; chain multiple inference steps with input/output from previous step  - &#x60;Splitter:&#x60; randomly routes to the target service according to the weight  - &#x60;Ensemble:&#x60; routes the request to multiple models and then merge the responses  - &#x60;Switch:&#x60; routes the request to one of the steps based on condition | [default to '']
**steps** | [**list[V1alpha1InferenceStep]**](V1alpha1InferenceStep.md) | Steps defines destinations for the current router node | [optional] 

[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)


