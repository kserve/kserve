apiVersion: serving.kserve.io/v1alpha1
kind: ClusterServingRuntime
metadata:
  name: kserve-lmcache-vllm
spec:
  annotations:
    prometheus.kserve.io/port: '8080'
    prometheus.kserve.io/path: "/metrics"
  supportedModelFormats:
    - name: huggingface
      version: "1"
      autoSelect: true
      priority: 2
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      image: lmcache/vllm-openai:2025-03-10
      args:
        - --model_name={{.Name}}
        - --kv-transfer-config
        - '{"kv_connector":"LMCacheConnector","kv_role":"kv_both"}'
        - --enable-chunked-prefill=false            # Chunked prefill is not supported by LMCache for now. https://github.com/LMCache/LMCache/pull/392
      env:
        - name: LMCACHE_USE_EXPERIMENTAL
          value: "True"
        - name: chunk_size
          value: 256                 # Default value
        
      securityContext:
        allowPrivilegeEscalation: false
        privileged: false
        runAsNonRoot: true
        capabilities:
          drop:
            - ALL
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
        limits:
          cpu: "1"
          memory: 2Gi
      volumeMounts:
        - name: devshm
          mountPath: /dev/shm
  volumes:
    - name: devshm
      emptyDir:
        medium: Memory
  hostIPC: false
